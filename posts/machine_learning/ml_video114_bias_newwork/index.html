<!doctype html><html lang=en dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="机器学习的四种paradigms # 连接主义(connectionist) aka 深度学习,神经网络. 认为表示学习非常重要. 需要学出来一个分布式的表示(distributed representation). Tensorflow PyTorch Theano Caffe 符号主义 Prolog 基于逻辑的学习 统计学习 基于统计学的基本假设 SVM, 统计学习理论, vc-dimention 概率图模型 GAN VAE 贝叶斯公式 # $P(A|B)=\frac{P(B|A) * P(A)}{P(B)}$
概率图模型 # 以高效的方法求变量的联合分布
graph TDEarthquake(Earthquake)-->Radio(Radio)Earthquake-->Alarm(Alarm)Burglary(Burglary)-->AlarmAlarm-->Call(Call)DAG(Directed Acyclic Graph) 有向无环图 Node-随机变量, Edges-边
上帝视角的P(B,E,A,R,C) $X_i$和$X_{ancestors}|X_{parents}$独立 $P(B,E,A,R,C)$ $=P(B)P(E|B)P(A|B,E)P(R|A,B,E)P(C|R,A,B,E)$ $=P(B)P(E)P(A|B,E)P(R|E)P(C|A)$
DAG能够告诉我们 如果我们有n个变量$x_i$,那么这些变的联合概率分布可以拆解为: $P(X_{1:n})=P(x_1,x_2,\cdots,x_n)=\prod_{i=1}^nP(x_i|parents(x_i))$
概率图模型求法 # 概率图模型的求法: 消元法 aka(动态规划) aka(分配律) 动态规划的思想在: 概率图模型, HMM, VC-dimension, RL中的MDP, 神经网络BP中的DP 这几块用过"><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="/posts/machine_learning/ml_video114_bias_newwork/"><meta property="og:title" content="机器学习-视频学习系列14-贝叶斯网络"><meta property="og:description" content="机器学习的四种paradigms # 连接主义(connectionist) aka 深度学习,神经网络. 认为表示学习非常重要. 需要学出来一个分布式的表示(distributed representation). Tensorflow PyTorch Theano Caffe 符号主义 Prolog 基于逻辑的学习 统计学习 基于统计学的基本假设 SVM, 统计学习理论, vc-dimention 概率图模型 GAN VAE 贝叶斯公式 # $P(A|B)=\frac{P(B|A) * P(A)}{P(B)}$ 概率图模型 # 以高效的方法求变量的联合分布 graph TDEarthquake(Earthquake)-->Radio(Radio)Earthquake-->Alarm(Alarm)Burglary(Burglary)-->AlarmAlarm-->Call(Call)DAG(Directed Acyclic Graph) 有向无环图 Node-随机变量, Edges-边 上帝视角的P(B,E,A,R,C) $X_i$和$X_{ancestors}|X_{parents}$独立 $P(B,E,A,R,C)$ $=P(B)P(E|B)P(A|B,E)P(R|A,B,E)P(C|R,A,B,E)$ $=P(B)P(E)P(A|B,E)P(R|E)P(C|A)$ DAG能够告诉我们 如果我们有n个变量$x_i$,那么这些变的联合概率分布可以拆解为: $P(X_{1:n})=P(x_1,x_2,\cdots,x_n)=\prod_{i=1}^nP(x_i|parents(x_i))$ 概率图模型求法 # 概率图模型的求法: 消元法 aka(动态规划) aka(分配律) 动态规划的思想在: 概率图模型, HMM, VC-dimension, RL中的MDP, 神经网络BP中的DP 这几块用过"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2019-06-07T23:33:54+00:00"><meta property="article:modified_time" content="2019-06-07T23:33:54+00:00"><meta property="article:tag" content="机器学习"><meta property="article:tag" content="贝叶斯网络"><title>机器学习-视频学习系列14-贝叶斯网络 | </title><link rel=manifest href=/manifest.json><link rel=icon href=/favicon.png><link rel=canonical href=/posts/machine_learning/ml_video114_bias_newwork/><link rel=stylesheet href=/book.min.309b7ed028807cdb68d8d61e26d609f48369c098dbf5e4d8c0dcf4cdf49feafc.css><script defer src=/fuse.min.js></script><script defer src=/en.search.min.8aa13e2f4b9fdd1fa1c269db82d4ba6bb2746e64391b4052adb11ab4309caef3.js></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span></span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu>
</label><strong>机器学习-视频学习系列14-贝叶斯网络</strong>
<label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><ul><li><a href=#机器学习的四种paradigms>机器学习的四种paradigms</a></li><li><a href=#贝叶斯公式>贝叶斯公式</a></li><li><a href=#概率图模型>概率图模型</a></li><li><a href=#概率图模型求法>概率图模型求法</a></li></ul></li></ul></nav></aside></header><article class="markdown book-post"><h1><a href=/posts/machine_learning/ml_video114_bias_newwork/>机器学习-视频学习系列14-贝叶斯网络</a></h1><h5>June 7, 2019</h5><div><a href=/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/>机器学习</a></div><div><a href=/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/>机器学习</a>,
<a href=/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C/>贝叶斯网络</a></div><h3 id=机器学习的四种paradigms>机器学习的四种paradigms
<a class=anchor href=#%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%9a%84%e5%9b%9b%e7%a7%8dparadigms>#</a></h3><ol><li>连接主义(connectionist) aka 深度学习,神经网络. 认为表示学习非常重要. 需要学出来一个分布式的表示(distributed representation). Tensorflow PyTorch Theano Caffe</li><li>符号主义 Prolog 基于逻辑的学习</li><li>统计学习 基于统计学的基本假设 SVM, 统计学习理论, vc-dimention</li><li>概率图模型 GAN VAE</li></ol><h3 id=贝叶斯公式>贝叶斯公式
<a class=anchor href=#%e8%b4%9d%e5%8f%b6%e6%96%af%e5%85%ac%e5%bc%8f>#</a></h3><p>$P(A|B)=\frac{P(B|A) * P(A)}{P(B)}$</p><h3 id=概率图模型>概率图模型
<a class=anchor href=#%e6%a6%82%e7%8e%87%e5%9b%be%e6%a8%a1%e5%9e%8b>#</a></h3><p>以高效的方法求变量的联合分布</p><script src=/mermaid.min.js></script><script>mermaid.initialize({flowchart:{useMaxWidth:!0},theme:"default"})</script><p class=mermaid>graph TD
Earthquake(Earthquake)-->Radio(Radio)
Earthquake-->Alarm(Alarm)
Burglary(Burglary)-->Alarm
Alarm-->Call(Call)</p><ol><li><p>DAG(Directed Acyclic Graph) 有向无环图
Node-随机变量, Edges-边</p></li><li><p>上帝视角的P(B,E,A,R,C)
$X_i$和$X_{ancestors}|X_{parents}$独立
$P(B,E,A,R,C)$
$=P(B)P(E|B)P(A|B,E)P(R|A,B,E)P(C|R,A,B,E)$
$=P(B)P(E)P(A|B,E)P(R|E)P(C|A)$</p></li><li><p>DAG能够告诉我们
如果我们有n个变量$x_i$,那么这些变的联合概率分布可以拆解为:
$P(X_{1:n})=P(x_1,x_2,\cdots,x_n)=\prod_{i=1}^nP(x_i|parents(x_i))$</p></li></ol><h3 id=概率图模型求法>概率图模型求法
<a class=anchor href=#%e6%a6%82%e7%8e%87%e5%9b%be%e6%a8%a1%e5%9e%8b%e6%b1%82%e6%b3%95>#</a></h3><p>概率图模型的求法: 消元法 aka(动态规划) aka(分配律)
动态规划的思想在: 概率图模型, HMM, VC-dimension, RL中的MDP, 神经网络BP中的DP 这几块用过</p></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><ul><li><a href=#机器学习的四种paradigms>机器学习的四种paradigms</a></li><li><a href=#贝叶斯公式>贝叶斯公式</a></li><li><a href=#概率图模型>概率图模型</a></li><li><a href=#概率图模型求法>概率图模型求法</a></li></ul></li></ul></nav></div></aside></main></body></html>