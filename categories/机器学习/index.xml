<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>机器学习 on</title><link>/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link><description>Recent content in 机器学习 on</description><generator>Hugo</generator><language>en</language><lastBuildDate>Sat, 08 Jun 2019 17:34:36 +0000</lastBuildDate><atom:link href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml"/><item><title>机器学习-视频学习系列15-隐马尔可夫模型</title><link>/posts/machine_learning/ml_video15_hidden_marcov_model/</link><pubDate>Sat, 08 Jun 2019 17:34:36 +0000</pubDate><guid>/posts/machine_learning/ml_video15_hidden_marcov_model/</guid><description>问题模型 # graph LRx1(x1)-->x2(x2)x2-->x3(x3)x3-->xn(xn)x1-->y1(y1)x2-->y2(y2)x3-->y3(y3)模型公式 # 目标: 求$P(x_t|y_{1:t})$ 步骤:
predict 求$P(x_t|y_{1:t-1})$ 使用到恒等变形公式: $P(A|BC)$ $= \frac{P(ABC)}{P(BC)}$ $= \frac{P(B|AC)P(A|C)P(C)}{P(B|C)P(C)}$ $= \frac{P(B|AC)P(A|C)}{P(B|C)}$
update 求$P(x_t|y_{1:t})$ 依旧使用上面的恒等变形公式: $P(x_t|y_{1:t}) = P(x_t|y_t,y_{t-1})$ $=$</description></item><item><title>机器学习-视频学习系列14-贝叶斯网络</title><link>/posts/machine_learning/ml_video114_bias_newwork/</link><pubDate>Fri, 07 Jun 2019 23:33:54 +0000</pubDate><guid>/posts/machine_learning/ml_video114_bias_newwork/</guid><description>机器学习的四种paradigms # 连接主义(connectionist) aka 深度学习,神经网络. 认为表示学习非常重要. 需要学出来一个分布式的表示(distributed representation). Tensorflow PyTorch Theano Caffe 符号主义 Prolog 基于逻辑的学习 统计学习 基于统计学的基本假设 SVM, 统计学习理论, vc-dimention 概率图模型 GAN VAE 贝叶斯公式 # $P(A|B)=\frac{P(B|A) * P(A)}{P(B)}$
概率图模型 # 以高效的方法求变量的联合分布
graph TDEarthquake(Earthquake)-->Radio(Radio)Earthquake-->Alarm(Alarm)Burglary(Burglary)-->AlarmAlarm-->Call(Call)DAG(Directed Acyclic Graph) 有向无环图 Node-随机变量, Edges-边
上帝视角的P(B,E,A,R,C) $X_i$和$X_{ancestors}|X_{parents}$独立 $P(B,E,A,R,C)$ $=P(B)P(E|B)P(A|B,E)P(R|A,B,E)P(C|R,A,B,E)$ $=P(B)P(E)P(A|B,E)P(R|E)P(C|A)$
DAG能够告诉我们 如果我们有n个变量$x_i$,那么这些变的联合概率分布可以拆解为: $P(X_{1:n})=P(x_1,x_2,\cdots,x_n)=\prod_{i=1}^nP(x_i|parents(x_i))$
概率图模型求法 # 概率图模型的求法: 消元法 aka(动态规划) aka(分配律) 动态规划的思想在: 概率图模型, HMM, VC-dimension, RL中的MDP, 神经网络BP中的DP 这几块用过</description></item><item><title>机器学习-视频学习系列12-聚类</title><link>/posts/machine_learning/ml_video12_aggregation/</link><pubDate>Fri, 07 Jun 2019 16:02:35 +0000</pubDate><guid>/posts/machine_learning/ml_video12_aggregation/</guid><description>聚类 # 基于对象的聚类(Objective based clustering) # K-means 聚类 # k-means 问题描述: 找到cpt1,cpt2,&amp;hellip;.cpt, 使 $ min\sum_{i=1}^n m_{j \in 1 \cdots k} d^2(x^i,c_j) $ k-median 问题描述: 找到cpt1,cpt2,&amp;hellip;.cpt, 使 $ min\sum_{i=1}^n m_{j \in 1 \cdots k} d(x^i,c_j) $ k-center 问题描述: 找到一个分类方式最小化最大半径 Lloyds method: 总是收敛的(目标函数单调递减,并且有下界, 该方法一定收敛) # 问题描述:
Input: 一组数据集 $ x^1, x^2, \cdots, x^n \in R^d$ Initialize: 1. 中心点 $c_1, c_2, \cdots, c_k \in R^d$ 2. 簇$C_1,C_2,\cdots,C_k$以随机方式聚合 Repeat:直到目标函数没有任何变化 1. for each j: $C_j \in {x \in S }$ where closest center is $c_j$ 2.</description></item></channel></rss>